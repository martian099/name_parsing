# Name & Address Parser for OCR-Scanned Documents

Extract structured data from messy OCR-scanned customer records using a fine-tuned DistilBERT NER model.

```python
from name_parsing import NameAddressParser

parser = NameAddressParser("models/onnx/quantized")
result = parser.parse("Alex or Mary Doe, 1201 Braddock Ave, Richmond VA, 22312")
# {'first_name': 'alex', 'last_name': 'doe', 'street_name': 'braddock'}
```

**Key features:**
- Handles OCR noise (character swaps, drops, doubles)
- Handles OCR-merged tokens (e.g., "37/harbor" instead of "37 Harbor")
- Extracts only the first person's name when multiple names appear
- Returns the most distinctive street name word (filters out "Ave", "St", etc.)
- CPU-only, ~10ms inference latency (p99)
- 67 MB quantized ONNX model — no GPU or PyTorch needed at runtime

## Quick Start

### 1. Set Up the Environment

```bash
# Clone the repo
git clone https://github.com/<your-username>/name-parsing.git
cd name-parsing

# Create the conda environment (all dependencies pinned)
conda env create -f environment.yaml

# Activate it
conda activate name-parsing

# Install the package in development mode
pip install -e .
```

### 2. Inference (Using the Pre-Trained Model)

If the repo includes a pre-trained model in `models/onnx/quantized/`, you can start parsing immediately:

```python
from name_parsing import NameAddressParser

parser = NameAddressParser("models/onnx/quantized")

# Single name
parser.parse("John Smith, 500 Oak Ave, Denver CO 80201")
# {'first_name': 'john', 'last_name': 'smith', 'street_name': 'oak'}

# Multiple names — extracts only the first person
parser.parse("Alex or Mary Doe, 1201 Braddock Ave, Richmond VA 22312")
# {'first_name': 'alex', 'last_name': 'doe', 'street_name': 'braddock'}

# OCR-merged tokens — handles gracefully
parser.parse("sarah martinez 37/harbor way coastal city, ca 90210")
# {'first_name': 'sarah', 'last_name': 'martinez', 'street_name': 'harbor'}

# Batch processing
parser.parse_batch(["John Smith, 100 Main St", "Jane Doe, 200 Oak Ave"])
```

### 3. Training From Scratch

If you want to retrain the model (e.g., with different data or parameters):

```bash
# Step 1: Generate synthetic training data (4000 examples)
python training/generate_training_data.py \
    --num-examples 4000 \
    --output data/raw/train.json \
    --seed 42

# Step 2: Fine-tune DistilBERT (takes ~6 min on CPU)
python training/train.py \
    --data data/raw/train.json \
    --output models/finetuned \
    --epochs 15 \
    --batch-size 16 \
    --lr 5e-5

# Step 3: Export to ONNX + quantize (265 MB → 67 MB)
python training/export_onnx.py \
    --model models/finetuned \
    --output models/onnx

# Step 4: Evaluate accuracy
python training/evaluate.py \
    --model models/onnx/quantized \
    --data data/raw/train.json \
    --max-examples 500
```

### 4. Run Tests

```bash
# Run all tests (unit tests + integration tests + benchmark)
pytest tests/ -v

# Just the postprocessor unit tests (no model needed)
pytest tests/test_postprocessor.py -v

# Integration tests (requires trained model in models/onnx/quantized/)
pytest tests/test_inference.py -v
```

## Project Structure

```
name-parsing/
├── src/name_parsing/          # Main package (used at inference time)
│   ├── __init__.py            # Exports NameAddressParser
│   ├── config.py              # Labels, paths, hyperparameters
│   ├── model.py               # NameAddressParser: tokenize → ONNX → postprocess
│   └── postprocessor.py       # Entity extraction, gap-aware joining, street filtering
│
├── training/                  # Training pipeline (run once)
│   ├── generate_training_data.py  # Synthetic data with OCR noise
│   ├── train.py               # Fine-tune DistilBERT
│   ├── export_onnx.py         # ONNX export + INT8 quantization
│   └── evaluate.py            # Per-field accuracy evaluation
│
├── tests/
│   ├── test_postprocessor.py  # 26 unit tests for post-processing logic
│   └── test_inference.py      # 8 integration tests + latency benchmark
│
├── notebooks/
│   ├── play.ipynb             # Interactive playground
│   └── examples.ipynb         # Usage examples and edge cases
│
├── models/                    # Model artifacts (generated by training)
│   ├── finetuned/             # PyTorch model checkpoint
│   └── onnx/
│       ├── onnx_export/       # Full-precision ONNX model
│       └── quantized/         # INT8 quantized model (used for inference)
│
├── data/raw/                  # Training data (generated)
│   └── train.json
│
├── environment.yaml           # Conda environment (all versions pinned)
├── pyproject.toml             # Package config + dependency groups
├── DOCUMENTATION.md           # Detailed technical documentation
└── README.md                  # This file
```

## How It Works

This project uses **Named Entity Recognition (NER)** with a fine-tuned **DistilBERT** model to label each subword token in the input text as one of:

```
O  B-FIRST_NAME  I-FIRST_NAME  B-LAST_NAME  I-LAST_NAME  B-STREET_NAME  I-STREET_NAME
```

The pipeline:
1. **Tokenize** the raw OCR text with WordPiece (no whitespace pre-splitting)
2. **Predict** a label for each subword token using the quantized ONNX model
3. **Post-process**: extract entities, join subwords (gap-aware), filter street names

The character-level tokenization approach is critical — it lets the model handle OCR-merged tokens (like "37/harbor") because WordPiece splits on learned subword boundaries, not whitespace. Each subword gets its own label, so the model can correctly identify "harbor" as a street name even when it's jammed against "37/".

For a deep dive into the architecture, design decisions, and implementation details, see [DOCUMENTATION.md](DOCUMENTATION.md).

## Runtime Dependencies

For **inference only**, the minimal dependencies are:

- `onnxruntime` — runs the quantized model (~30 MB)
- `transformers` — provides the WordPiece tokenizer
- `numpy`

No PyTorch needed at runtime. Total footprint: ~67 MB model + ~50 MB libraries.

## Performance

| Metric | Value |
|--------|-------|
| Training F1 | 99.75% |
| first_name accuracy | 100% |
| last_name accuracy | 100% |
| street_name accuracy | 99.8% |
| Inference latency (p99) | ~10ms |
| Model size (quantized) | 67 MB |

## License

MIT
